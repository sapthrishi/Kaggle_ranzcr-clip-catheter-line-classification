{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# !pip install timm","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!cp ../input/resnet200d-public/resnet200d_320_CV9632.pth ./resnet200d_fold0.pth","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install GPUtil","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ../input/pytorch-images-seresnet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nsys.path.append ('../input/pytorch-images-seresnet')\n\nimport os\nimport gc\nimport time\nimport math\nimport random\nimport datetime\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n\nimport torch\nimport torch.nn as nn\nfrom   torch.nn import CrossEntropyLoss, MSELoss\nfrom   torch.nn.modules.loss import _WeightedLoss\nimport torch.nn.functional as F\nfrom   torch.nn import Parameter\nfrom   torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\nfrom   transformers import TrainingArguments, Trainer, AdamW, get_linear_schedule_with_warmup\n\nfrom   fastai.losses import LabelSmoothingCrossEntropy\n\n# from   warmup_scheduler import GradualWarmupScheduler\nfrom   sklearn import preprocessing\nfrom   sklearn.metrics import accuracy_score\nfrom   sklearn.model_selection import StratifiedKFold, GroupKFold\nfrom   sklearn.metrics import accuracy_score, precision_recall_fscore_support, matthews_corrcoef, roc_auc_score\nfrom   sklearn.metrics import mean_squared_error, mean_absolute_error, explained_variance_score, r2_score\n\nimport timm\nimport albumentations as A\nfrom   albumentations.pytorch import ToTensorV2\nfrom   albumentations.core.transforms_interface import DualTransform\nfrom   albumentations.augmentations import functional as AF\nimport cv2\n\nfrom   tqdm import tqdm\nfrom   pprint import pprint\nfrom   functools import partial\nimport matplotlib.pyplot as plt\nfrom   GPUtil import showUtilization as gpu_usage\nfrom   numba import cuda\nimport warnings\nwarnings.filterwarnings (\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CFG:\n    device       = torch.device ('cuda' if torch.cuda.is_available () else 'cpu')\n    num_workers  = 4\n    model_name   = 'resnet200d_320' # ['deit_base_patch16_224', 'vit_base_patch16_384', 'resnext50_32x4d', 'tf_efficientnet_b7_ns']\n    size         = 640  # [64, 128, 224, 384, 512, 640, 720]\n    train        = True\n    freeze       = True     # this is updated during training   \n    freeze_epo   = 0.5       # float: these many epochs are with frozen model at the beginning\n    epochs       = 1 \n    epochsNx     = 4\n    criterion    = 'BCEWithLogitsLoss'    # ['CrossEntropyLoss', 'BCEWithLogitsLoss', 'SmoothBCEwithLogits']\n    batch_size   = 1 #[10, 32, 64]\n    weight_decay = 1e-6\n    max_grad_norm= 1\n    seed         = 42\n    target_size  = -1    # init below\n    n_fold       = 50\n    train_fold   = [0] #, 1, 2, 3, 4]\n    # infer_fold = [0, 1, 2, 3, 4]\n    print_every  = 100\n    img_ext      = '.jpg'\n    img_col      = \"StudyInstanceUID\"\n    label_cols   = [\n                    'ETT - Abnormal', 'ETT - Borderline', 'ETT - Normal',\n                    'NGT - Abnormal', 'NGT - Borderline', 'NGT - Incompletely Imaged', 'NGT - Normal', \n                    'CVC - Abnormal', 'CVC - Borderline', 'CVC - Normal',\n                    'Swan Ganz Catheter Present'\n    ]\n    model_infer_path_prefix = \".\" # \"../input/ranzcr-chest-xray-pretrainer\" \n    model_train_path_prefix = \".\"\n    train_path   = '../input/ranzcr-clip-catheter-line-classification/train'\n    train_csv    = '../input/ranzcr-clip-catheter-line-classification/train.csv'\n    test_path    = '../input/ranzcr-clip-catheter-line-classification/test'\n    test_csv     = '../input/ranzcr-clip-catheter-line-classification/sample_submission.csv'\n    output_dir   = 'Output/'\n    \n    prev_target_size = 14\n    tta          = 3 \n    min_unfreez_layer = 2 # allowed to unfreeze layers 11 to 5 only and not less than 5\n    smoothing    = 0.1\n    freeze_epo   = 1 # after these epochs, gradually unfreeze top layers\n    gradual_unfreez_epo = 7\n    IMG_MEAN     = [0.485, 0.456, 0.406] #Mean for normalization Transform cassava = [0.4303, 0.4967, 0.3134] imgnet = [0.485, 0.456, 0.406]\n    IMG_STD      = [0.229, 0.224, 0.225] #STD for normalization Transform cassava = [0.2142, 0.2191, 0.1954] imgnet = [0.229, 0.224, 0.225]            \n    \nCFG.target_size = len (CFG.label_cols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def init_logger (log_file=CFG.output_dir+'train.log'):\n    \n    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n    logger = getLogger (__name__)\n    logger.setLevel (INFO)\n    handler1 = StreamHandler ()\n    handler1.setFormatter (Formatter (\"%(message)s\"))\n    handler2 = FileHandler (filename=log_file)\n    handler2.setFormatter (Formatter (\"%(message)s\"))\n    logger.addHandler (handler1)\n    logger.addHandler (handler2)\n    return logger","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything (seed):\n    \n    random.seed (seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed (seed)\n    torch.manual_seed (seed)\n    torch.cuda.manual_seed (seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    return","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Augmentation Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# NOTE: We don't normalize here since it all gets dark\n# if advprop:           # for models using advprop pretrained weights\n#     normalize = transforms.Lambda(lambda img: img * 2.0 - 1.0)\n# else:\n#     normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    \nAug_Norm = A.Normalize ()\n\ntrain_transforms = A.Compose ([\n    A.Resize (CFG.size, CFG.size),\n    A.HorizontalFlip (p=0.5),\n    A.HueSaturationValue (hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n    A.RandomBrightnessContrast (brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n    A.augmentations.transforms.RGBShift (r_shift_limit=20, g_shift_limit=20, b_shift_limit=20, always_apply=False, p=0.5),\n    A.augmentations.transforms.ChannelDropout (channel_drop_range=(1, 1), fill_value=0, always_apply=False, p=0.5),\n    A.augmentations.transforms.GridDistortion (num_steps=5, distort_limit=0.3, interpolation=1, border_mode=4, value=None, mask_value=None, always_apply=False, p=0.5),\n    A.CoarseDropout(p=0.5),\n    Aug_Norm,\n    ToTensorV2 (p=1.0),\n])\nvalid_transforms = A.Compose ([\n    A.Resize (CFG.size, CFG.size),\n    Aug_Norm,\n    ToTensorV2 (p=1.0),\n])\n\ndef get_transforms (data='train'):\n    \n    if 'train' in data:\n        return train_transforms\n    elif 'valid' in data:\n        return valid_transforms\n    else:\n        return valid_transforms","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ImgDataset (Dataset):\n    \n    def __init__(self, df, img_file_colname=CFG.img_col, label_cols=CFG.label_cols, \n                 transform=get_transforms(), img_dir=CFG.train_path, img_ext=CFG.img_ext):\n        \n        super ().__init__()\n        self.df               = df.reset_index (drop=True)\n        self.img_ext          = CFG.img_ext\n        self.img_dir          = img_dir\n        self.label_cols       = label_cols\n        self.img_file_colname = img_file_colname\n        self.transform        = transform\n        return\n    \n    def __len__(self):\n        return self.df.shape[0]\n    \n    def __getitem__(self, idx):\n        \n        file_name = self.df[self.img_file_colname][idx].replace (self.img_ext, '') + self.img_ext\n        file_path = f'{self.img_dir}/{file_name}'\n        image     = cv2.imread (file_path)                              #;print (file_path)\n        image     = cv2.cvtColor (image, cv2.COLOR_BGR2RGB)\n        if self.transform:\n            image = self.transform (image=image)['image'].float ()\n        else:\n            image = ToTensorV2 ()(image = image)[\"image\"].float ()\n        \n        if len (self.label_cols) > 0:\n            label = torch.tensor (self.df.loc[idx, self.label_cols]).float () # long ()\n            return image, label\n        return image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def getFolds ():\n    \n    train_folds_df = pd.read_csv (CFG.train_csv)\n    label = train_folds_df[CFG.label_cols]\n    if len (CFG.label_cols) > 1:\n        label = train_folds_df[CFG.label_cols[0]]\n        \n    skf = StratifiedKFold (n_splits=CFG.n_fold, shuffle=True, random_state=CFG.seed)\n    for n, (train_index, val_index) in enumerate (skf.split (train_folds_df, label)):\n        train_folds_df.loc[val_index, 'fold'] = int (n)\n    train_folds_df['fold'] = train_folds_df['fold'].astype (int)\n    # print (train_folds_df.groupby (['fold', label]).size ())\n        \n    return train_folds_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_imgs (dataset_show):\n    \n    from pylab import rcParams\n    rcParams['figure.figsize'] = 20,10\n    for i in range (2):\n        f, axarr = plt.subplots (1,5)\n        for p in range (5):\n            idx = np.random.randint (0, len (dataset_show))\n            img, label = dataset_show[idx]                         # ;print (img.size()) ;print (label)\n            img = img.byte ()\n            axarr[p].imshow (img.permute(1, 2, 0))\n            axarr[p].set_title (idx)\n    return\n\nTR_DATASET = ImgDataset (getFolds ())\nplot_imgs (TR_DATASET)\ndel TR_DATASET\ngc.collect ()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> # Loss Functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"# this works for only 1 label, not multi-label target.\nclass SmoothBCEwLogits (_WeightedLoss):\n    \n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n            self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_criterion (class_wt):\n    \n    if CFG.criterion=='CrossEntropyLoss':\n        criterion = LabelSmoothingCrossEntropy ()\n    elif CFG.criterion=='SmoothBCEwithLogits':\n        criterion = SmoothBCEwLogits (smoothing=CFG.smoothing)\n    elif CFG.criterion=='BCEWithLogitsLoss':\n        criterion =  nn.BCEWithLogitsLoss (pos_weight=class_wt)\n    return criterion","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ResNet200D (nn.Module):\n    \n    def __init__(self, model_name='resnet200d_320'):\n        \n        super().__init__()\n        self.model = timm.create_model (model_name, pretrained=False)\n        n_features = self.model.fc.in_features\n        self.model.global_pool = nn.Identity ()\n        self.model.fc = nn.Identity ()\n        self.pooling = nn.AdaptiveAvgPool2d (1)\n        self.fc = nn.Linear (n_features, 11)\n\n    def forward(self, x):\n        \n        bs = x.size(0)\n        features = self.model(x)\n        pooled_features = self.pooling(features).view(bs, -1)\n        output = self.fc(pooled_features)\n        return output\n    \n    def freeze (self):\n        # To freeze the residual layers\n        for param in self.model.parameters ():\n            param.requires_grad = False\n\n        for param in self.fc.parameters ():\n            param.requires_grad = True\n        return\n    \n    def unfreeze (self):\n        # Unfreeze all layers\n        for param in self.model.parameters ():\n            param.requires_grad = True\n        for param in self.fc.parameters ():\n            param.requires_grad = True\n        return","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_state (model_path, model):\n    \n    state_dict = None\n    try:  # single GPU model_file\n        state_dict = torch.load (model_path, map_location=torch.device ('cpu'))['model']\n        # print (state_dict)\n        model.load_state_dict (state_dict) # (torch.load (model_path, map_location=torch.device ('cpu')), strict=True)\n    except:  # multi GPU model_file\n        state_dict = torch.load (model_path, map_location=torch.device ('cpu'))\n        state_dict = {k[7:] if k.startswith ('module.') else k: state_dict[k] for k in state_dict.keys ()}\n        model.load_state_dict (state_dict)\n    return state_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def getModel (fold, isTrain=True):\n    \n    model = ResNet200D ()\n    if isTrain:\n        \n        # TODO: _infer_ to _train_,   _end to _maxacc\n        model_path = f'{CFG.model_infer_path_prefix}/{CFG.model_name}_fold{fold}.pth' #_end.pth'        \n        print (\"loading\", model_path)\n        load_state (model_path, model)        \n    else:\n        \n        # TODO: change CFG.model_train_path_prefix to CFG.model_infer_path_prefix\n        model_path = f'{CFG.model_train_path_prefix}/{CFG.model_name}_fold{fold}_maxacc.pth'\n        load_state (model_path, model)\n        \n    if CFG.freeze:        \n        model.freeze ()\n    # else:\n    #     model.unfreeze ()\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Helpers"},{"metadata":{"trusted":true},"cell_type":"code","source":"def format_time (elapsed):\n    '''\n    Takes a time in seconds and returns a string hh:mm:ss\n    '''\n    \n    # Round to the nearest second.\n    elapsed_rounded = int(round((elapsed)))\n    \n    # Format as hh:mm:ss\n    return str (datetime.timedelta (seconds=elapsed_rounded))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sigmoid (x):  \n    return np.exp (-np.logaddexp (0, -x))\n\ndef compute_metrics (labels, pred_pr):\n    \n    preds   = pred_pr.argmax (-1)             #;print ('labels.shape=', labels.shape, 'preds.shape=', preds.shape, 'pred_logits.shape=', pred_logits.shape)\n    precision, recall, f1, _ = precision_recall_fscore_support (labels, preds, average='macro')\n    acc     = accuracy_score (labels, preds)\n    mcc     = matthews_corrcoef (labels, preds)   # matthews correlation coefficient\n    auc     = -1\n    try:\n        auc = roc_auc_score (labels, pred_pr[:, 1])\n    except:\n        pass\n    metrics = {\n        'mcc'      : mcc,\n        'accuracy' : acc,\n        'f1'       : f1,\n        'precision': precision,\n        'recall'   : recall,\n        'auc'      : auc\n    }\n    return metrics\n\ndef compute_multilabel_binary_metrics (labels, logits):\n    \n    pred_pr = sigmoid (logits)    \n    metrics = []\n    n_class = labels.shape[1]\n    for i in range (n_class):\n        \n        label  = labels[:, i]\n        prob1  = pred_pr[:, i]\n        prob0  = 1 - prob1\n        pred_p = np.hstack ((prob0.reshape ((-1, 1)), prob1.reshape ((-1, 1))))\n        scores = compute_metrics (label, pred_p)\n        metrics.append (scores)\n        \n    # Now Avg over each classes\n    metrics_df = pd.DataFrame (metrics)  \n    auc = list (metrics_df['auc'].values)\n    auc = np.mean ([a for a in auc if a >= 0])\n    metrics_df.drop (columns=['auc'], inplace=True)\n    metrics_df = metrics_df.mean ()\n    metrics_df['auc'] = auc\n    return metrics_df.to_dict ()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class MyTrainer:\n    \n    def __init__(self, fold, model, args, train_dataset, eval_dataset, criterion, compute_metrics=compute_multilabel_binary_metrics):\n        \n        self.fold            = fold\n        self.model           = model.to (CFG.device)\n        self.args            = args\n        self.train_dataset   = train_dataset\n        self.eval_dataset    = eval_dataset\n        self.criterion       = criterion\n        self.compute_metrics = compute_metrics\n        self.isTrained       = False\n        self.device          = CFG.device\n        self.optimizer       = AdamW (model.parameters (), lr=args.learning_rate, eps=args.adam_epsilon)\n        self.epochs          = self.args.num_train_epochs\n        self.train_dataloader, self.validation_dataloader, self.lr_scheduler, self.num_training_steps = self.get_dataLoaders ()\n        return\n    \n    def get_dataLoaders (self):        \n        \n        # Create the DataLoaders for our training and validation sets.\n        if isinstance (self.train_dataset, torch.utils.data.IterableDataset):\n            train_sampler = None\n        else:\n            train_sampler = RandomSampler (self.train_dataset)           # Better use RandomSampler\n        train_dataloader  = DataLoader (\n                    self.train_dataset,                                  # The training samples.\n                    sampler     = train_sampler,                           \n                    batch_size  = self.args.per_device_train_batch_size,\n                    num_workers = CFG.num_workers,\n                    pin_memory  = True\n        )\n        # train_dataloader  = DataLoader (self.train_dataset, batch_size=self.args.per_device_train_batch_size) # TODO: comment this\n        validation_dataloader = None\n        if self.eval_dataset:            \n            # For validation the order doesn't matter, so we'll just read them sequentially.\n            validation_dataloader = DataLoader (\n                        self.eval_dataset,             # The validation/dev samples.\n                        sampler     = SequentialSampler (self.eval_dataset),\n                        batch_size  = self.args.per_device_eval_batch_size,\n                        num_workers = CFG.num_workers,\n                        pin_memory  = False\n            )\n            # validation_dataloader  = DataLoader (self.eval_dataset, batch_size=self.args.per_device_eval_batch_size) # TODO: comment this\n            \n        # Total number of training steps is [number of batches] x [number of epochs]. \n        # (Note that this is not the same as the number of training samples).\n        num_training_steps = len (train_dataloader) * self.epochs\n\n        # Create the learning rate scheduler.\n        lr_scheduler = get_linear_schedule_with_warmup (self.optimizer, \n                        num_warmup_steps   = self.args.warmup_steps, # Default value in run_glue.py\n                        num_training_steps = num_training_steps)\n        return train_dataloader, validation_dataloader, lr_scheduler, num_training_steps\n    \n    def test_iterate_dataloader (self):\n        \n        for step, batch in enumerate (self.train_dataloader):\n            print (step)\n            print (batch)\n            break\n        return\n    \n    \n    def train (self):\n        \n        seed_everything (seed=CFG.seed)\n        training_stats   = []\n        min_val_loss     = 9999\n        min_train_loss   = 9999\n        max_val_auc      = -1        \n        step = 0\n        \n        for epoch_i in range (0, self.epochs):\n            \n            avg_train_loss   = 0\n            total_train_loss = 0\n            print('======== Epoch {:} / {:} ========'.format (epoch_i + 1, self.epochs))\n            t0 = time.time ()\n            total_train_loss = 0\n            self.model.train ()\n            for stp, batch in enumerate (self.train_dataloader): \n                \n                # if step ==  int (CFG.freeze_epo * len(self.train_dataloader)): \n                #     print (\"Unfreezing the model\")\n                #     self.model.unfreeze ()\n                if step % CFG.print_every == 0:\n                    elapsed = format_time (time.time() - t0)\n                    print ('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format (step, len (self.train_dataloader), elapsed))\n                if (self.args.max_steps > 0 and self.args.max_steps < step) or  \\\n                   (self.args.eval_steps> 0 and step % self.args.eval_steps==0):                    \n                    if step > 0:\n                        avg_train_loss = total_train_loss / step\n                    training_time = format_time (time.time () - t0)\n                    \n                    if self.validation_dataloader:\n                        print (\"Running Validation...\")\n                        avg_val_loss, avg_val_f1, avg_val_mcc, avg_val_auc, avg_val_precision, avg_val_recall, avg_val_accuracy, validation_time = self.evaluate ()\n                        training_stats.append ({\n                                'epoch'         : epoch_i + 1,\n                                'training_loss' : avg_train_loss,\n                                'eval_loss'     : avg_val_loss,\n                                'eval_f1'       : avg_val_f1,\n                                'eval_mcc'      : avg_val_mcc, \n                                'eval_precision': avg_val_precision,\n                                'eval_recall'   : avg_val_recall,\n                                'eval_auc'      : avg_val_auc, \n                                'eval_accuracy' : avg_val_accuracy,\n                                'training_time' : training_time,\n                                'eval_time'     : validation_time                   \n                        })\n                        \n                        # save this model if the eval loss decreases from the minimum so far\n                        if avg_val_loss < min_val_loss: \n                            min_val_loss = avg_val_loss\n                            torch.save (self.model.state_dict (), f\"{CFG.model_train_path_prefix}/{CFG.model_name}_fold{self.fold}_min_val_loss.pth\")\n                    \n                    if self.args.max_steps > 0 and self.args.max_steps < step :                        \n                        print (\"\")\n                        print (\"Training complete!\")\n                        print (\"Total training took {:} (h:mm:ss)\".format (format_time (time.time ()-total_t0)))\n                        self.isTrained = True\n                        self.plot_train_stats_regression (training_stats)\n                        return training_stats\n                    \n                self.model.zero_grad ()                        \n                images = batch[0].to (self.device)\n                labels = batch[1].to (self.device)\n                gpu_usage ()\n                logits = self.model (images)\n                loss   = self.criterion (logits, labels)\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), CFG.max_grad_norm)\n                loss.backward ()\n                self.optimizer.step ()\n                self.lr_scheduler.step ()\n                total_train_loss += loss.item ()\n                step += 1\n            # epoch end\n                        \n            avg_train_loss = total_train_loss / len (self.train_dataloader)\n            # Measure how long this epoch took.\n            training_time = format_time (time.time () - t0)            \n            print (\"  Average training loss: {0:.2f}\".format (avg_train_loss))\n            print (\"  Training epcoh took: {:}\".format (training_time))            \n            if self.validation_dataloader:\n                \n                print (\"\\n  Running Validation...\")\n                avg_val_loss, avg_val_f1, avg_val_mcc, avg_val_auc, avg_val_precision, avg_val_recall, avg_val_accuracy, validation_time = self.evaluate ()\n                # Record all statistics from this epoch.\n                training_stats.append ({\n                        'epoch'         : epoch_i + 1,\n                        'training_loss' : avg_train_loss,\n                        'eval_loss'     : avg_val_loss,\n                        'eval_f1'       : avg_val_f1,\n                        'eval_mcc'      : avg_val_mcc, \n                        'eval_precision': avg_val_precision,\n                        'eval_recall'   : avg_val_recall,\n                        'eval_auc'      : avg_val_auc, \n                        'eval_accuracy' : avg_val_accuracy,\n                        'training_time' : training_time,\n                        'eval_time'     : validation_time                   \n                })\n                # save this epoch's model if the eval loss decreases from the minimum so far\n                if avg_val_loss < min_val_loss:                    \n                    min_val_loss = avg_val_loss\n                    torch.save (self.model.state_dict (), f\"{CFG.model_train_path_prefix}/{CFG.model_name}_fold{self.fold}_min_val_loss.pth\")\n                if avg_val_auc > max_val_auc:\n                    max_val_auc = avg_val_auc\n                    torch.save (self.model.state_dict (), f\"{CFG.model_train_path_prefix}/{CFG.model_name}_fold{self.fold}_max_val_auc.pth\")\n            else:\n                \n                training_stats.append ({\n                    'epoch'         : epoch_i + 1,\n                    'training_loss' : avg_train_loss,\n                    'training_time' : training_time,\n                })\n                if avg_train_loss < min_train_loss: \n                    \n                    min_train_loss = avg_train_loss\n                    torch.save (self.model.state_dict (), f\"{CFG.model_train_path_prefix}/{CFG.model_name}_fold{self.fold}_min_train_loss.pth\")\n                \n        print (\"\")\n        print (\"Training complete!\")\n        print (\"Total training took {:} (h:mm:ss)\".format (format_time (time.time ()-total_t0)))\n        self.isTrained = True\n        self.model.eval ()\n        try:\n            self.plot_train_stats (training_stats)\n        except:\n            pass\n        torch.save (self.model.state_dict (), f\"{CFG.model_train_path_prefix}/{CFG.model_name}_fold{self.fold}_end.pth\")\n        return training_stats\n    \n    \n    def evaluate (self):\n        \n        t0 = time.time ()\n        # Put the model in evaluation mode--the dropout layers behave differently\n        # during evaluation.\n        self.model.eval ()\n\n        # Tracking variables \n        total_eval_mcc       = 0\n        total_eval_f1        = 0\n        total_eval_precision = 0\n        total_eval_recall    = 0\n        total_eval_auc       = 0\n        total_eval_accuracy  = 0\n        total_eval_loss      = 0\n        nb_eval_steps        = 0\n\n        # Evaluate data for one epoch\n        for batch in self.validation_dataloader:\n            with torch.no_grad ():\n                \n                images = batch[0].to (self.device)\n                labels = batch[1]\n                logits = self.model (images).detach ().cpu ()\n                loss   = self.criterion (logits, labels)\n            \n            total_eval_loss += loss.item ()\n            logits     = logits.detach ().cpu ().numpy ()\n\n            # Calculate the accuracy for this batch of test sentences, and\n            # accumulate it over all batches.\n            metrics               = self.compute_metrics (labels, logits)\n            total_eval_mcc       += metrics['mcc']\n            total_eval_f1        += metrics['f1']\n            total_eval_precision += metrics['precision']\n            total_eval_recall    += metrics['recall']\n            total_eval_auc       += metrics['auc']\n            total_eval_accuracy  += metrics['accuracy']\n        # epoch end\n        \n        # Report the final accuracy for this validation run.\n        avg_val_f1 = total_eval_f1 / len (self.validation_dataloader)\n        print (\"  F1: {0:.3f}\".format (avg_val_f1))\n        avg_val_mcc = total_eval_mcc / len (self.validation_dataloader)\n        print (\"  MCC: {0:.3f}\".format (avg_val_mcc))\n        avg_val_precision = total_eval_precision / len (self.validation_dataloader)\n        print (\"  Precision: {0:.3f}\".format (avg_val_precision))\n        avg_val_recall = total_eval_recall / len (self.validation_dataloader)\n        print (\"  Recall: {0:.3f}\".format (avg_val_recall))\n        avg_val_auc = total_eval_auc / len (self.validation_dataloader)\n        print (\"  AUC: {0:.3f}\".format (avg_val_auc))\n        avg_val_accuracy = total_eval_accuracy / len (self.validation_dataloader)\n        print (\"  Accuracy: {0:.3f}\".format (avg_val_accuracy))\n        # Calculate the average loss over all of the batches.\n        avg_val_loss = total_eval_loss / len (self.validation_dataloader)\n        # Measure how long the validation run took.\n        validation_time = format_time (time.time () - t0)\n        print (\"  Validation Loss: {0:.2f}\".format (avg_val_loss))\n        print (\"  Validation took: {:}\".format (validation_time))            \n        return avg_val_loss, avg_val_f1, avg_val_mcc, avg_val_auc, avg_val_precision, avg_val_recall, avg_val_accuracy, validation_time\n    \n    \n    def plot_train_stats (self, training_stats):\n        \"\"\"\n        Draw Classification Report curve\n        \"\"\"\n        \n        mccs   = accuracies = f1_scores = precisions = recalls = auc = losses = epochs = -1\n        epochs = len (training_stats)\n        if 'eval_mcc' in training_stats[0]:\n            mccs       = [e['eval_mcc'] for e in training_stats]\n            sns.lineplot (x=np.arange(1, epochs + 1), y=mccs,       label='val_mcc')\n        if 'eval_accuracy' in training_stats[0]:\n            accuracies = [e['eval_accuracy'] for e in training_stats]\n            sns.lineplot (x=np.arange(1, epochs + 1), y=accuracies, label='val_accuracy')\n        if 'eval_f1' in training_stats[0]:\n            f1_scores  = [e['eval_f1'] for e in training_stats]\n            sns.lineplot (x=np.arange(1, epochs + 1), y=f1_scores,  label='val_f1') \n        if 'eval_precision' in training_stats[0]:\n            precisions = [e['eval_precision'] for e in training_stats]\n            sns.lineplot (x=np.arange(1, epochs + 1), y=precisions, label='val_precision')\n        if 'eval_recall' in training_stats[0]:\n            recalls    = [e['eval_recall'] for e in training_stats]\n            sns.lineplot (x=np.arange(1, epochs + 1), y=recalls,    label='val_recall')\n        if 'eval_auc' in training_stats[0]:\n            auc        = [e['eval_auc'] for e in training_stats]\n            sns.lineplot (x=np.arange(1, epochs + 1), y=mccs,       label='val_auc')\n        if 'eval_loss' in training_stats[0]:\n            losses     = [e['eval_loss'] for e in training_stats]\n        if 'training_loss'  in training_stats[0]:\n            tr_losses  = [e['training_loss'] for e in training_stats]\n            sns.lineplot (x=np.arange(1, epochs + 1), y=tr_losses,  label='tr_losses')\n            \n        plt.show ()\n        print ('mccs       :', mccs)\n        print ('accuracies :', accuracies)\n        print ('precisions :', precisions)\n        print ('recalls    :', recalls)\n        print ('f1_scores  :', f1_scores)\n        print ('auc        :', auc)\n        print ('losses     :', losses)\n        print ('tr_losses  :', tr_losses)\n        \n        \n        plt.plot (recalls, precisions, marker='.', label='Prcision-Recall Curve')\n        # axis labels\n        plt.xlabel ('Recall')\n        plt.ylabel ('Precision')\n        # show the legend\n        plt.legend ()\n        # show the plot\n        plt.show ()\n        return\n    \n    \n    def getTrainedModel (self):\n        \n        if self.isTrained:\n            return self.model\n        return None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def free_gpu_cache():\n    \n    print(\"Initial GPU Usage\")\n    gpu_usage()                             \n\n    torch.cuda.empty_cache()\n\n    cuda.select_device(0)\n    cuda.close()\n    cuda.select_device(0)\n\n    print(\"GPU Usage after emptying the cache\")\n    gpu_usage()\n    return\n\n# free_gpu_cache()           ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"training_args = TrainingArguments (\n    \n    learning_rate = 5e-5,\n    max_grad_norm = 1.0,\n    output_dir='./results',          # output directory\n    overwrite_output_dir=True,\n    num_train_epochs=3,              # total # of training epochs\n    per_device_train_batch_size=64,  # batch size per device during training\n    per_device_eval_batch_size=120,  # batch size for evaluation\n    warmup_steps=0,                  # number of warmup steps for learning rate scheduler\n    weight_decay=0.01,               # strength of weight decay\n    eval_steps = 100,\n    save_steps = 100,\n    logging_dir='./logs',            # directory for storing logs    \n    save_total_limit=1000,\n    # save_steps=int (len (train_dataset)/32),\n    # fp16=True,\n)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_fold_loop (fold, train_df=getFolds ()):\n\n    print (f\"========== fold: {fold} training ==========\")\n\n    trn_idx        = train_df[train_df['fold'] != fold].index\n    val_idx        = train_df[train_df['fold'] == fold].index\n    train_folds_df = train_df.loc[trn_idx].reset_index (drop=True)\n    valid_folds_df = train_df.loc[val_idx].reset_index (drop=True) \n    class_wt       = train_df[CFG.label_cols].sum (axis=0)\n    class_wt       = torch.tensor (np.sum (np.array (class_wt.values)) / np.array (class_wt.values))  ;print ('class_wt =', class_wt)\n    class_wt       = class_wt.to (CFG.device)\n    del train_df; gc.collect ()\n    \n    criterion      = get_criterion (class_wt=class_wt)\n    model          = getModel (fold, isTrain=True)\n    model          = model.float()\n    train_dataset  = ImgDataset (train_folds_df, transform=get_transforms ('train'))\n    valid_dataset  = ImgDataset (valid_folds_df, transform=get_transforms ('valid'))    \n    trainer        = MyTrainer (\n        fold            = fold,\n        model           = model,   \n        args            = training_args,\n        train_dataset   = train_dataset,\n        eval_dataset    = None, # valid_dataset,\n        criterion       = criterion,\n        compute_metrics = compute_multilabel_binary_metrics,\n    )\n    gpu_usage ()\n    metrics = trainer.train ()\n    return metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_main ():\n    \n    print (f\"========== train_main() ==========\")\n    if CFG.train:        \n        valid_scores = []\n        for fold in range (CFG.n_fold):\n            if fold in CFG.train_fold:\n                \n                valid_scores_fold = train_fold_loop (fold)\n                valid_scores_fold = np.array (valid_scores_fold).reshape ((1, -1))\n                valid_scores.append (valid_scores_fold)\n                \n        LOGGER.info (f\"========== CV ==========\")\n        valid_scores = np.vstack (valid_scores)\n        valid_scores = np.mean (valid_scores, axis=0)\n        print (\"CV Scores =\", valid_scores)\n    return","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!mkdir -p /kaggle/working/Output/\n!touch /kaggle/working/Output/train.log\ngc.collect ()\n# model_names = timm.list_models (pretrained=True)\n# model_names = timm.list_models ('*resnet*', pretrained=True)\n# pprint (model_names)\n\nLOGGER = init_logger ()\nseed_everything (seed=CFG.seed)\ntrain_main ()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print ('Done !')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}