{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ../input/resnet200d-320-640-fold0-colab-enx7/resnet200d_320_640_fold0_colab_eNx7.pth","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!cp ../input/resnet200d-320-640-fold0-colab-enx7/resnet200d_320_640_fold0_colab_eNx7.pth ./resnet200d_320_fold0_end.pth","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import sys\nsys.path.append ('../input/pytorch-images-seresnet')\n\nimport os\nimport gc\nimport pandas as pd\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nfrom   torch.nn import CrossEntropyLoss, MSELoss\nfrom   torch.nn.modules.loss import _WeightedLoss\nimport torch.nn.functional as F\nfrom   torch.nn import Parameter\nimport math\n\nimport fastai\nfrom   fastai.callback.mixup import MixUp, CutMix\nfrom   fastai.callback import *\nfrom   fastai.callback.all import *\nfrom   fastai.callback.training import GradientClip\nfrom   fastai.callback.all import SaveModelCallback, EarlyStoppingCallback, ReduceLROnPlateau \nfrom   fastai.data.core import *\nfrom   fastai.data.load import *\nfrom   fastai.learner import Learner\nfrom   fastai.metrics import *\nfrom   fastai.optimizer import OptimWrapper \nfrom   fastai.losses import LabelSmoothingCrossEntropy\n\n# from   warmup_scheduler import GradualWarmupScheduler\nfrom   sklearn import preprocessing\nfrom   sklearn.metrics import accuracy_score\nfrom   sklearn.model_selection import StratifiedKFold, GroupKFold\n\nimport timm\nimport albumentations as A\nfrom   albumentations.pytorch import ToTensorV2\nfrom   albumentations.core.transforms_interface import DualTransform\nfrom   albumentations.augmentations import functional as AF\nimport cv2\n\nfrom   tqdm import tqdm\nfrom   pprint import pprint\nfrom   functools import partial\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings (\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CFG:\n    device          = torch.device ('cuda' if torch.cuda.is_available () else 'cpu')\n    num_workers     = 4\n    model_name      = 'resnet200d_320' # ['deit_base_patch16_224', 'vit_base_patch16_384', 'resnext50_32x4d', 'tf_efficientnet_b7_ns']\n    size            = 640  # [64, 128, 224, 384, 512, 640, 720]\n    train           = True\n    freeze          = True     # this is updated during training   \n    epochs          = 1 \n    epochsNx        = 3\n    criterion       = 'BCEWithLogitsLoss'    # ['CrossEntropyLoss', 'BCEWithLogitsLoss', 'SmoothBCEwithLogits']\n    batch_size      = 112\n    weight_decay    = 1e-6\n    max_grad_norm   = 1000.0\n    seed            = 42\n    target_size     = -1    # init below\n    n_fold          = 50\n    aug_diff_thresh = 1\n    train_fold      = [0] #, 1, 2, 3, 4]\n    # infer_fold    = [0, 1, 2, 3, 4]\n    img_ext         = '.jpg'\n    img_col         = \"StudyInstanceUID\"\n    label_cols      = [\n                    'ETT - Abnormal', 'ETT - Borderline', 'ETT - Normal',\n                    'NGT - Abnormal', 'NGT - Borderline', 'NGT - Incompletely Imaged', 'NGT - Normal', \n                    'CVC - Abnormal', 'CVC - Borderline', 'CVC - Normal',\n                    'Swan Ganz Catheter Present'\n    ]\n    model_infer_path_prefix = \".\" # \"../input/ranzcr-chest-xray-pretrainer\" \n    model_train_path_prefix = \".\"\n    train_path          = '../input/ranzcr-clip-catheter-line-classification/train'\n    train_csv           = '../input/ranzcr-clip-catheter-line-classification/train.csv'\n    test_path           = '../input/ranzcr-clip-catheter-line-classification/test'\n    test_csv            = '../input/ranzcr-clip-catheter-line-classification/sample_submission.csv'\n    aug_train_path      = './aug_train'\n    aug_train_csv       = './aug_train.csv'\n    output_dir          = 'Output/'\n    tta                 = 3         # train time error data augmentations\n    \n    freeze_epo          = 0.5       # float: these many epochs are with frozen model at the beginning\n    print_every         = 100\n    prev_target_size    = 14\n    min_unfreez_layer   = 2 # allowed to unfreeze layers 11 to 5 only and not less than 5\n    smoothing           = 0.1\n    freeze_epo          = 1 # after these epochs, gradually unfreeze top layers\n    gradual_unfreez_epo = 7\n    IMG_MEAN            = [0.485, 0.456, 0.406] #Mean for normalization Transform cassava = [0.4303, 0.4967, 0.3134] imgnet = [0.485, 0.456, 0.406]\n    IMG_STD             = [0.229, 0.224, 0.225] #STD for normalization Transform cassava = [0.2142, 0.2191, 0.1954] imgnet = [0.229, 0.224, 0.225]            \n    \nCFG.target_size = len (CFG.label_cols)\n!mkdir ./aug_train\n!touch ./aug_train.csv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def init_logger (log_file=CFG.output_dir+'train.log'):\n    \n    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n    logger = getLogger (__name__)\n    logger.setLevel (INFO)\n    handler1 = StreamHandler ()\n    handler1.setFormatter (Formatter (\"%(message)s\"))\n    handler2 = FileHandler (filename=log_file)\n    handler2.setFormatter (Formatter (\"%(message)s\"))\n    logger.addHandler (handler1)\n    logger.addHandler (handler2)\n    return logger","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def seed_everything (seed):\n    \n    random.seed (seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed (seed)\n    torch.manual_seed (seed)\n    torch.cuda.manual_seed (seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    return","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Augmentation Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# NOTE: We don't normalize here since it all gets dark\n# if advprop:           # for models using advprop pretrained weights\n#     normalize = transforms.Lambda(lambda img: img * 2.0 - 1.0)\n# else:\n#     normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    \nAug_Norm = A.Normalize ()\n\ntrain_transforms = A.Compose ([\n    A.Resize (CFG.size, CFG.size),\n    A.HorizontalFlip (p=0.5),\n    A.HueSaturationValue (hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n    A.RandomBrightnessContrast (brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n    A.augmentations.transforms.RGBShift (r_shift_limit=20, g_shift_limit=20, b_shift_limit=20, always_apply=False, p=0.5),\n    A.augmentations.transforms.ChannelDropout (channel_drop_range=(1, 1), fill_value=0, always_apply=False, p=0.5),\n    A.augmentations.transforms.GridDistortion (num_steps=5, distort_limit=0.3, interpolation=1, border_mode=4, value=None, mask_value=None, always_apply=False, p=0.5),\n    A.CoarseDropout(p=0.5),\n    Aug_Norm,\n    ToTensorV2 (p=1.0),\n])\nvalid_transforms = A.Compose ([\n    A.Resize (CFG.size, CFG.size),\n    Aug_Norm,\n    ToTensorV2 (p=1.0),\n])\n\ndef get_transforms (data='train'):\n    \n    if 'train' in data:\n        return train_transforms\n    elif 'valid' in data:\n        return valid_transforms\n    else:\n        return valid_transforms","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ImgDataset (Datasets):\n    \n    # at inference time set label_cols=[]\n    def __init__(self, df, img_file_colname=CFG.img_col, label_cols=CFG.label_cols,\n                 transform=get_transforms(), img_dir=CFG.train_path, img_ext=CFG.img_ext,\n                 aug_img_dir=CFG.aug_train_path):\n        \n        super (ImgDataset, self).__init__(df.reset_index(drop=True), tfms=None, n_inp=1)\n        self.img_ext          = CFG.img_ext\n        self.img_dir          = img_dir\n        self.label_cols       = label_cols\n        self.img_file_colname = img_file_colname\n        self.transform        = transform\n        return\n    \n    def __len__(self):\n        return self.items.shape[0]    \n    \n    def __getitem__(self, idx):\n        \n        file_path = None\n        file_name = self.items[self.img_file_colname][idx]\n        if file_name.endswith ('.pt'):\n            file_path = file_name      # f'{self.aug_img_dir}/{file_name}'\n            image     = torch.load (file_path).float ()         #;print ('aug_img.shape =', image.shape)\n        else:\n            file_name = file_name.replace (self.img_ext, '') + self.img_ext\n            file_path = f'{self.img_dir}/{file_name}'\n            image     = cv2.imread (file_path)\n            image     = cv2.cvtColor (image, cv2.COLOR_BGR2RGB)\n            if self.transform:\n                image = self.transform (image=image)['image'].float ()\n            else:\n                image = ToTensorV2 ()(image = image)[\"image\"].float ()\n        \n        if len (self.label_cols) > 0:\n            label = torch.tensor (self.items.loc[idx, self.label_cols]).float () # long ()\n            return image, label\n        return image","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def getFolds (isOnlyAug=False):\n    \n    train_folds_df = pd.DataFrame ()\n    if not isOnlyAug:\n        train_folds_df = pd.read_csv (CFG.train_csv)   #;CFG.n_fold=10;train_folds_df=train_folds_df.iloc[:10, :]\n    if os.path.exists (CFG.aug_train_csv):\n        try:\n            aug_df = pd.read_csv (CFG.aug_train_csv)\n            train_folds_df = train_folds_df.append (aug_df)\n            # print (\"aug_df =\", aug_df)\n        except:\n            pass\n    train_folds_df.reset_index (drop=True, inplace=True)\n    label = train_folds_df[CFG.label_cols]\n    if len (CFG.label_cols) > 1:\n        label = train_folds_df[CFG.label_cols[0]]\n        \n    skf = StratifiedKFold (n_splits=CFG.n_fold, shuffle=True, random_state=CFG.seed)\n    for n, (train_index, val_index) in enumerate (skf.split (train_folds_df, label)):\n        train_folds_df.loc[val_index, 'fold'] = int (n)\n    train_folds_df['fold'] = train_folds_df['fold'].astype (int)\n    # print (train_folds_df.groupby (['fold', label]).size ())\n        \n    return train_folds_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_imgs (dataset_show):\n    \n    from pylab import rcParams\n    rcParams['figure.figsize'] = 20,10\n    for i in range (2):\n        f, axarr = plt.subplots (1,5)\n        for p in range (5):\n            idx = np.random.randint (0, len (dataset_show))\n            img, label = dataset_show[idx]                         # ;print (img.size()) ;print (label)\n            img = img.byte ()\n            axarr[p].imshow (img.permute(1, 2, 0))\n            axarr[p].set_title (idx)\n    return\n\nTR_DATASET = ImgDataset (getFolds ())\nplot_imgs (TR_DATASET)\ndel TR_DATASET\ngc.collect ()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> # Loss Functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"# this works for only 1 label, not multi-label target.\nclass SmoothBCEwLogits (_WeightedLoss):\n    \n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n            self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_criterion (class_wt):\n    \n    if CFG.criterion=='CrossEntropyLoss':\n        criterion = LabelSmoothingCrossEntropy ()\n    elif CFG.criterion=='SmoothBCEwithLogits':\n        criterion = SmoothBCEwLogits (smoothing=CFG.smoothing)\n    elif CFG.criterion=='BCEWithLogitsLoss':\n        criterion =  nn.BCEWithLogitsLoss (pos_weight=class_wt)\n    return criterion","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ResNet200D_320 (nn.Module):\n    \n    def __init__(self, model_name='resnet200d_320'):\n        \n        super().__init__()\n        self.model = timm.create_model (model_name, pretrained=False)\n        n_features = self.model.fc.in_features\n        self.model.global_pool = nn.Identity ()\n        self.model.fc = nn.Identity ()\n        self.pooling = nn.AdaptiveAvgPool2d (1)\n        self.fc = nn.Linear (n_features, 11)\n\n    def forward(self, x):\n        \n        bs = x.size(0)\n        features = self.model(x)\n        pooled_features = self.pooling(features).view(bs, -1)\n        output = self.fc(pooled_features)\n        return output\n    \n    def freeze (self):\n        # To freeze the residual layers\n        for param in self.model.parameters ():\n            param.requires_grad = False\n\n        for param in self.fc.parameters ():\n            param.requires_grad = True\n        return\n    \n    def unfreeze (self):\n        # Unfreeze all layers\n        for param in self.model.parameters ():\n            param.requires_grad = True\n        for param in self.fc.parameters ():\n            param.requires_grad = True\n        return\n    \nclass CustomResNet(nn.Module):\n    def __init__(self, model_name=CFG.model_name, pretrained=False):\n        super().__init__()\n        self.model = timm.create_model(model_name, pretrained=pretrained)\n        n_features = self.model.fc.in_features\n        self.model.fc = nn.Linear (n_features, CFG.target_size)\n        return\n    \n    def forward(self, x):\n        x = self.model (x)\n        return x\n    \n    def freeze (self):\n        # To freeze the residual layers\n        for param in self.model.parameters():\n            param.requires_grad = False\n\n        for param in self.model.fc.parameters():\n            param.requires_grad = True\n        return\n    \n    def unfreeze(self):\n        # Unfreeze all layers\n        for param in self.model.parameters():\n            param.requires_grad = True\n        return\n    \nclass CustomResNext(nn.Module):\n    def __init__(self, model_name=CFG.model_name, pretrained=False):\n        super().__init__()\n        self.model = timm.create_model(model_name, pretrained=pretrained)\n        n_features = self.model.fc.in_features\n        self.model.fc = nn.Linear(n_features, CFG.target_size)\n\n    def forward(self, x):\n        x = self.model(x)\n        return x\n    \n    def freeze(self):\n        # To freeze the residual layers\n        for param in self.model.parameters():\n            param.requires_grad = False\n\n        for param in self.model.fc.parameters():\n            param.requires_grad = True\n    \n    def unfreeze(self):\n        # Unfreeze all layers\n        for param in self.model.parameters():\n            param.requires_grad = True\n            \n\nclass CustomEfficientNet(nn.Module):\n    def __init__(self, model_name=CFG.model_name, pretrained=False):\n        super().__init__()\n        self.model = timm.create_model (CFG.model_name, pretrained=pretrained)\n        n_features = self.model.classifier.in_features\n        self.model.classifier = nn.Linear(n_features, CFG.target_size)\n\n    def forward(self, x):\n        x = self.model(x)\n        return x\n        \n    def freeze(self):\n        # To freeze the residual layers\n        for param in self.model.parameters():\n            param.requires_grad = False\n\n        for param in self.model.classifier.parameters():\n            param.requires_grad = True\n    \n    def unfreeze(self):\n        # Unfreeze all layers\n        for param in self.model.parameters():\n            param.requires_grad = True\n    \n    \nclass CustomDeiT(nn.Module):\n    def __init__(self, model_name=CFG.model_name, pretrained=False):\n        super().__init__()\n        self.model = torch.hub.load('facebookresearch/deit:main', model_name, pretrained=pretrained)\n        n_features = self.model.head.in_features\n        self.model.head = nn.Linear(n_features, CFG.target_size)\n\n    def forward(self, x):\n        x = self.model(x)\n        return x\n    \n    def freeze(self):\n        # To freeze the residual layers\n        for param in self.model.parameters():\n            param.requires_grad = False\n\n        for param in self.model.head.parameters():\n            param.requires_grad = True\n    \n    def unfreeze(self):\n        # Unfreeze all layers\n        for param in self.model.parameters():\n            param.requires_grad = True\n\n    \nclass CustomViT(nn.Module):\n    \n    def __init__(self, model_name=CFG.model_name, pretrained=False, \n                ): # min_unfreez_layer=CFG.min_unfreez_layer, max_layer_no=CFG.max_layer_no):\n        \n        super().__init__()\n        self.model      = timm.create_model(model_name, pretrained=pretrained)\n        n_features      = self.model.head.in_features\n        self.model.head = nn.Linear(n_features, CFG.target_size)\n        # self.min_unfreez_layer = min_unfreez_layer\n        # self.max_layer_no      = max_layer_no\n        return\n\n    def forward(self, x):\n        x = self.model(x)\n        return x\n    \n    def freeze(self):\n        # To freeze the residual layers\n        for param in self.model.parameters():\n            param.requires_grad = False\n\n        for param in self.model.head.parameters():\n            param.requires_grad = True\n        return\n    \n    def unfreeze(self):\n        # Unfreeze all layers\n        for param in self.model.parameters():\n            param.requires_grad = True\n        return\n    \n    # def unfreeze_layer (self, layer_no=11):\n    #     # unfreeze a particular layer\n    #     if layer_no >= self.min_unfreez_layer and layer_no <= self.max_layer_no:\n    #         for param in self.model.blocks[layer_no].parameters ():\n    #             param.requires_grad = False\n    #     return","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from fastai.imports import *\nfrom fastai.torch_core import *\nfrom fastai.learner import *\n    \n@patch\n@delegates (subplots)\ndef plot_metrics (self: Recorder, nrows=None, ncols=None, figsize=None, **kwargs):\n    \n    metrics = np.stack(self.values)\n    names = self.metric_names[1:-1]\n    n = len(names) - 1\n    if nrows is None and ncols is None:\n        nrows = int(math.sqrt(n))\n        ncols = int(np.ceil(n / nrows))\n    elif nrows is None: nrows = int(np.ceil (n / ncols))\n    elif ncols is None: ncols = int(np.ceil (n / nrows))\n    figsize = figsize or (ncols * 6, nrows * 4)\n    fig, axs = subplots (nrows, ncols, figsize=figsize, **kwargs)\n    axs = [ax if i < n else ax.set_axis_off() for i, ax in enumerate (axs.flatten())][:n]\n    for i, (name, ax) in enumerate (zip (names, [axs[0]] + axs)):\n        ax.plot (metrics[:, i], color='#1f77b4' if i == 0 else '#ff7f0e', label='valid' if i > 0 else 'train')\n        ax.set_title (name if i > 1 else 'losses')\n        ax.legend (loc='best')\n    plt.show ()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@delegates (torch.optim.AdamW.__init__)\ndef pytorch_AdamW (param_groups, **kwargs):\n    return OptimWrapper (torch.optim.AdamW ([{'params': ps, **kwargs} for ps in param_groups]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class UnfreezeCallback (Callback):\n    \n    def __init__(self, freeze_epo=CFG.freeze_epo):\n        super().__init__()\n        self.freeze_epo   = freeze_epo\n        return\n        \n    def before_epoch (self): \n        if self.epoch == self.freeze_epo:\n            print ('UnfreezeCallback : unfreezing the model')\n            self.learn.model.unfreeze ()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# the dataloader shoud be shuffle=False when using this !!!!\nclass AugmentCallback (Callback):\n    \n    def __init__(self, train_path=CFG.train_path, aug_train_path=CFG.aug_train_path,\n                 aug_train_csv=CFG.aug_train_csv, tta=CFG.tta, img_col=CFG.img_col, \n                 label_cols=CFG.label_cols, batch_size=CFG.batch_size):\n        super().__init__()\n        self.train_path    = train_path\n        self.aug_train_csv = aug_train_csv\n        self.img_col       = img_col\n        self.label_cols    = label_cols\n        self.batch_size    = batch_size\n        self.tta           = tta\n        self.aug_train_path= aug_train_path\n        return\n        \n    def after_batch (self):\n        \n        # print ('AugmentCallback : creating new augmented train data for wrong preds')        \n        pred   = (torch.sigmoid (self.pred) >= 0.5) + 0            #;print ('pred   =', pred)\n        diff   = torch.sum (self.y - pred, axis=1)                 #;print ('diff   =', diff)\n        which  = torch.abs (diff) >= CFG.aug_diff_thresh\n        if torch.sum (which+0) == 0:\n            return\n        \n        # save these image tensors and their entry in the train csv file\n        files  = []\n        labels = []\n        for i in range (len (which)):\n            if which[i] == True:\n                \n                index = self.iter * self.batch_size + i\n                for j in range (self.tta):\n                    \n                    image, label = self.dl.dataset[index]\n                    image, label = image.cpu (), label.cpu ()\n                    file  = f\"{self.aug_train_path}/aug_{index}_{j}.pt\"\n                    torch.save (image, file)\n                    files.append (file)\n                    labels.append (label.numpy ().reshape((1, -1)))\n        \n        df = pd.DataFrame ()\n        df[self.img_col]    = files\n        df[self.label_cols] = np.vstack (labels)\n        aug_df = pd.DataFrame ()\n        if os.path.exists (self.aug_train_csv):\n            try:\n                aug_df = pd.read_csv (self.aug_train_csv)\n            except:\n                pass\n        aug_df = aug_df.append (df)\n        aug_df.to_csv (self.aug_train_csv, index=False)\n        # print (aug_df)\n        return","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_state (model_path, model):\n    \n    state_dict = None\n    try:  # single GPU model_file\n        state_dict = torch.load (model_path, map_location=torch.device ('cpu'))['model']\n        # print (state_dict)\n        model.load_state_dict (state_dict) # (torch.load (model_path, map_location=torch.device ('cpu')), strict=True)\n    except:  # multi GPU model_file\n        state_dict = torch.load (model_path, map_location=torch.device ('cpu'))\n        state_dict = {k[7:] if k.startswith ('module.') else k: state_dict[k] for k in state_dict.keys ()}\n        model.load_state_dict (state_dict)\n    return state_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def getModel (fold, isTrain=True):\n    \n    model = None\n    if CFG.model_name == 'resnet200d_320':\n        model = ResNet200D_320 ()\n    elif 'deit_' in CFG.model_name:\n        model = CustomDeiT (model_name=CFG.model_name, pretrained=isTrain)\n    elif 'vit_' in CFG.model_name:\n        model = CustomViT (model_name=CFG.model_name, pretrained=isTrain)\n    elif 'resnext' in CFG.model_name:\n        model = CustomResNext (CFG.model_name, pretrained=isTrain)\n    elif 'resnet' in CFG.model_name:\n        model = CustomResNet (CFG.model_name, pretrained=False)  #TODO: pretrained=isTrain\n    elif 'efficientnet' in CFG.model_name:\n        model = CustomEfficientNet (CFG.model_name, pretrained=isTrain)  \n    if isTrain:\n        \n        # TODO: _infer_ to _train_,   _end to _maxacc\n        model_path = f'{CFG.model_infer_path_prefix}/{CFG.model_name}_fold{fold}_end.pth'        \n        # if os.path.exists (model_path):\n        print (\"loading\", model_path)\n        load_state (model_path, model)\n    else:\n        \n        # TODO: change CFG.model_train_path_prefix to CFG.model_infer_path_prefix\n        model_path = f'{CFG.model_train_path_prefix}/{CFG.model_name}_fold{fold}_maxacc.pth'\n        load_state (model_path, model)\n        \n    if CFG.freeze:        \n        model.freeze ()\n    else:\n        model.unfreeze ()\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fit_epochs (fold, callbacks, learn, max_acc, epochNx, start_lr=1e-6, end_lr=5e-5, lr_scaler=1, model=None):\n    \n    if model is None:\n        model   = getModel (fold, isTrain=True).float().to (CFG.device)\n        learn.model = model\n    \n    # lr_min    = 0.00002\n    lr_min, _   = learn.lr_find (start_lr=start_lr, end_lr=end_lr, num_it=100) \n    lr_min      = lr_min * lr_scaler\n    print ('lr_min =', lr_min)\n    lr          = lr_min\n    learn.fit_one_cycle (CFG.epochs, lr, wd=CFG.weight_decay, cbs=callbacks)\n    # learn.recorder.plot_metrics ()\n    # learn.recorder.plot_lr ()        \n    acc = learn.recorder.metrics[0].value #.numpy ()\n    if acc > max_acc:\n        max_acc = acc\n        print (\"Max-Acc:\", max_acc)\n        # learn.save (f'{CFG.model_train_path_prefix}/{CFG.model_name}_fold{fold}_maxacc')\n    print ('saving:', f'{CFG.model_train_path_prefix}/{CFG.model_name}_fold{fold}_eNx{epochNx+1}.pth')\n    learn.save (f'{CFG.model_train_path_prefix}/{CFG.model_name}_fold{fold}_eNx{epochNx+1}')\n    torch.save (learn.model.state_dict (), f'{CFG.model_train_path_prefix}/{CFG.model_name}_fold{fold}_eNx{epochNx+1}.pt')\n    return max_acc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_DLs_LossFn (fold, shuffle=True, isOnlyAug=False):\n    \n    train_df       = getFolds (isOnlyAug=isOnlyAug)\n    trn_idx        = train_df[train_df['fold'] != fold].index\n    val_idx        = train_df[train_df['fold'] == fold].index\n    train_folds_df = train_df.loc[trn_idx].reset_index (drop=True)\n    valid_folds_df = train_df.loc[val_idx].reset_index (drop=True)\n    class_wt       = 1 / (1 + np.array (train_df[CFG.label_cols].sum (axis=0).values))\n    class_wt       = torch.tensor (class_wt / np.sum (class_wt)).to (CFG.device)   ;print ('class_wt =', class_wt)\n    del train_df; gc.collect ()\n    \n    loss_func      = get_criterion (class_wt=class_wt)    \n    train_dataset  = ImgDataset (train_folds_df, transform=get_transforms ('train'))\n    valid_dataset  = ImgDataset (valid_folds_df, transform=get_transforms ('valid'))\n    train_datlder  = DataLoader (train_dataset, batch_size=CFG.batch_size, shuffle=shuffle, device=CFG.device)\n    valid_datlder  = DataLoader (valid_dataset, batch_size=CFG.batch_size, shuffle=False,   device=CFG.device)\n    dls            = DataLoaders (train_datlder, valid_datlder, device=CFG.device)\n    return dls, loss_func","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_fold_loop (fold):\n    \n    print (f\"========== fold: {fold} training ==========\")    \n    metrics        = [accuracy, error_rate]                 # fbeta, auc_roc_score \n    if len (CFG.label_cols) > 1:\n        metrics    = [F1ScoreMulti (), accuracy_multi]\n    model          = getModel (fold, isTrain=True)\n    model          = model.float().to (CFG.device)\n    # modelfile    = f'{CFG.model_train_path_prefix}/{CFG.model_name}_fold{fold}'    \n    \n    # initially cfg.freeze=True, but only for the 1st time, at end it is =False to train the deep layers\n    dls       = None\n    loss_func = None\n    learn     = None\n    max_acc   = 0\n    for e_i in range (CFG.epochsNx):\n        \n        callbacks = [\n                # monitor: train_loss  valid_loss   train_f1_score  valid_f1_score   valid_accuracy_multi\n                # EarlyStoppingCallback (monitor='train_f1_score', min_delta=0.001, patience=3), # useless now\n                # SaveModelCallback     (monitor='train_f1_score', fname=modelfile),             # useless now\n                # UnfreezeCallback (),\n                ReduceLROnPlateau (monitor='train_f1_score', min_delta=0.001, factor=2.0, min_lr=1e-8, patience=1),\n                GradientClip (CFG.max_grad_norm),\n        ]\n        \n        # in epoch 1 we augment the data\n        # for epoch 2 onwards ++ augmented data hence create correspondng new loaders and learner\n        if e_i == 0:\n            \n            callbacks.append (AugmentCallback ())\n            dls, loss_func = get_DLs_LossFn (fold, shuffle=False)\n            learn          = Learner (dls, model, loss_func=loss_func, model_dir=f'{CFG.model_train_path_prefix}',\n                                 opt_func=partial (pytorch_AdamW, lr=0.007, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01),\n                                 metrics=metrics)\n            print (learn.summary ())\n            valid_scores = learn.validate (dl=dls.valid)\n            print (f'fold-{fold} before epochsNx-{e_i+1} valid-scores:-')\n            print (valid_scores)\n            # if unfrozen, decrease the batch_size for next epochs\n            # CFG.batch_size = CFG.batch_size * 0.7\n        elif e_i == 1:\n            \n            dls, loss_func = get_DLs_LossFn (fold, shuffle=True, isOnlyAug=True)\n            learn          = Learner (dls, model, loss_func=loss_func, model_dir=f'{CFG.model_train_path_prefix}',\n                                 opt_func=partial (pytorch_AdamW, lr=0.007, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01),\n                                 metrics=metrics)\n            # if e_i == 1:\n            #     CFG.freeze = False\n            #     print (\"Unfreezing the model\")\n            #     model.unfreeze ()\n            \n        learn.recorder.train_metrics = True\n        gc.collect ()        \n        lr_scaler = 0.9\n        start_lr, end_lr = 1e-7, 5e-5        \n        # For pre-training pass the model\n        max_acc = fit_epochs (fold, callbacks, learn, max_acc, e_i, start_lr, end_lr, lr_scaler, model)\n    # epochsNx end\n    \n    valid_scores = learn.validate (dl=dls.valid)\n    print (f'fold-{fold} epochsNx-end valid-scores:-')\n    print (valid_scores)\n    return valid_scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train_main ():\n    \n    if CFG.train:\n        \n        valid_scores = []\n        for fold in range (CFG.n_fold):\n            if fold in CFG.train_fold:\n                valid_scores_fold = train_fold_loop (fold)\n                valid_scores_fold = np.array (valid_scores_fold).reshape ((1, -1))\n                valid_scores.append (valid_scores_fold)\n                \n        print (f\"========== CV ==========\")\n        valid_scores = np.vstack (valid_scores)\n        valid_scores = np.mean (valid_scores, axis=0)\n        print (\"CV Scores =\", valid_scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!mkdir -p /kaggle/working/Output/\n!touch /kaggle/working/Output/train.log\ngc.collect ()\n# model_names = timm.list_models (pretrained=True)\n# model_names = timm.list_models ('*resnet*', pretrained=True)\n# pprint (model_names)\n\nLOGGER = init_logger ()\nseed_everything (seed=CFG.seed)\ntrain_main ()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print ('Done !')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}